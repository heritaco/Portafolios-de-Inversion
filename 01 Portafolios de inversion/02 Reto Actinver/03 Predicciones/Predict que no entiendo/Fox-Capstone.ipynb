{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "# Udacity Machine Learning Engineer Nanodegree\n",
    "## Capstone Project: \n",
    "## Predicting the Daily Direction of the S&P500\n",
    "\n",
    "### Completed and Submitted by Stephen Fox\n",
    "### October 2016\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "# Read data from SP500_historical.csv - data contains returns for the S&P500 and VIX index going back over 23 years\n",
    "# Source: Yahoo Finance\n",
    "\n",
    "all_data = pd.read_csv(\"SP500_historical.csv\",header=0)\n",
    "print (\"Market data read successfully!\")\n",
    "print (\"Number of data points:\", len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the column headers\n",
    "print (all_data.dtypes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the 'Date' column data from a string into a form that can be manipulated mathematically\n",
    "# Calculate the 'Days_Since_Open' feature, which is a measure of how many days have passed since the market last opened\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "all_data['Date'] = pd.to_datetime(all_data['Date'])\n",
    "\n",
    "all_data['Days_Since_Open'] = (all_data['Date'] - all_data['Date'].shift(-1))\n",
    "all_data['Days_Since_Open'] = all_data['Days_Since_Open'].astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize some of the key data\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_data = all_data.SP_Close\n",
    "plot_data_VIX = all_data.Vix_Close\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(2, 1)\n",
    "\n",
    "axes[0].axis([5962,0,400,2200])\n",
    "axes[0].plot(plot_data)\n",
    "axes[0].set_ylabel('S&P500 Close')\n",
    "axes[0].set_yticks([400,800,1200,1600,2000,2400])\n",
    "\n",
    "axes[1].axis([5962,0,0,100])\n",
    "axes[1].plot(plot_data_VIX)\n",
    "axes[1].set_ylabel('VIX Close')\n",
    "plt.xlabel('Trading Days Since Most Recent Day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_data_Volume = all_data.SP_Volume / 1000000\n",
    "\n",
    "plt.axis([5962,0,14,12000])\n",
    "plt.plot(plot_data_Volume)\n",
    "plt.ylabel('S&P500 Volume (in millions)')\n",
    "plt.xlabel('Trading Days Since Most Recent Day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Break_Coming' feature, which is a measure of whether the market will close for a break on the next day\n",
    "\n",
    "all_data['Break_Coming'] = all_data['Days_Since_Open'].shift(1)\n",
    "\n",
    "all_data.loc[all_data['Break_Coming']<=1,'Break_Coming'] = 0\n",
    "all_data.loc[all_data['Break_Coming']>1,'Break_Coming'] = 1\n",
    "\n",
    "all_data.Break_Coming.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Overnight_Return' feature\n",
    "\n",
    "all_data['Overnight_Return'] = all_data['SP_Open'] / all_data['SP_Close'].shift(-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 'No_Overnight_Change' value. This is not a feature, but was used to identify unreliable data rows\n",
    "# See accompanying report for further details\n",
    "\n",
    "all_data['No_Overnight_Change'] = all_data['Overnight_Return']\n",
    "\n",
    "all_data.loc[all_data['No_Overnight_Change']==0.00,'No_Overnight_Change'] = 1\n",
    "all_data.loc[all_data['No_Overnight_Change']!=1,'No_Overnight_Change'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Overnight_VIX' feature, which measures whether the VIX has opened up or down relative to previous day\n",
    "\n",
    "all_data['Overnight_VIX'] = all_data['Vix_Open'] / all_data['Vix_Close'].shift(-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the 'O_to_O' feature, which compares the current opening price to the previous day\n",
    "\n",
    "all_data['O_to_O'] = all_data['SP_Open'] / all_data['SP_Open'].shift(-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate 11 different trailing return features, 9 based on the S&P500 data and 2 based on VIX data\n",
    "\n",
    "all_data['Trail_1d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-2) - 1\n",
    "all_data['Trail_2d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-3) - 1\n",
    "all_data['Trail_3d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-4) - 1\n",
    "all_data['Trail_4d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-5) - 1\n",
    "all_data['Trail_5d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-6) - 1\n",
    "all_data['Trail_21d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-22) - 1\n",
    "all_data['Trail_63d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-64) - 1\n",
    "all_data['Trail_126d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-127) - 1\n",
    "all_data['Trail_252d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-253) - 1\n",
    "\n",
    "all_data['Trail_1d_VIX'] = all_data['Vix_Close'].shift(-1) / all_data['Vix_Close'].shift(-2) - 1\n",
    "all_data['Trail_5d_VIX'] = all_data['Vix_Close'].shift(-1) / all_data['Vix_Close'].shift(-6) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate 2 different trailing volume features, which measure whether the last day's volume is above or below\n",
    "# recent averages\n",
    "\n",
    "all_data['Trail_1d_Rel_Vol'] = all_data['SP_Volume'].shift(-1) / (\n",
    "    all_data['SP_Volume'].rolling(window=4).mean().shift(-5)) - 1\n",
    "\n",
    "all_data['Trail_5d_Rel_Vol'] = all_data['SP_Volume'].rolling(window=5).mean().shift(-5) / (\n",
    "    all_data['SP_Volume'].rolling(window=21).mean().shift(-26)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Trail_1d_PtT' feature, which measure how much the S&P500 moved from high to low values on the \n",
    "# previous trading day\n",
    "\n",
    "all_data['Trail_1d_PtT'] = (all_data['SP_High'].shift(-1) - all_data['SP_Low'].shift(-1)) / (0.5*(\n",
    "        all_data['SP_Open'].shift(-1) + all_data['SP_Close'].shift(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Trail_1d_VIX_PtT' feature, which measure how much the VIX moved from high to low values on the \n",
    "# previous trading day\n",
    "\n",
    "all_data['Trail_1d_VIX_PtT'] = (all_data['Vix_High'].shift(-1) - all_data['Vix_Low'].shift(-1)) / (0.5*(\n",
    "        all_data['Vix_Open'].shift(-1) + all_data['Vix_Close'].shift(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Intraday_Increase' label, which is ultimately what the machine learner should predict, and as is a\n",
    "# measure of whether the S&P500 increased on a given day (value = 1) or not (value = 0)\n",
    "\n",
    "all_data['Intraday_Increase'] = all_data['SP_Close'] - all_data['SP_Open']\n",
    "\n",
    "all_data.loc[all_data['Intraday_Increase']>0,'Intraday_Increase'] = 1\n",
    "all_data.loc[all_data['Intraday_Increase']<=0,'Intraday_Increase'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now going to delete those rows that have N/A values (e.g. you cannot do a 252 day trailing return if there are fewer than 252 days trailing a given data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noNA_data = all_data.dropna()\n",
    "\n",
    "print (\"Number of data points:\", len(noNA_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also delete rows where 'No_Overnight_Change' is equal to 1. These are points where the opening value is IDENTICAL to the previous closing data, which likely reflects missing true opening prices (the chances of the market opening at the identical level as the previous close are slim to none). Most of these points came from the older portion of the data set. There were almost no points like this in the most recent 10 years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validated_data = noNA_data[noNA_data.No_Overnight_Change != 1]\n",
    "print (\"Number of data points:\", len(validated_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I need to drop columns that would allow the machine learning algorithm to 'know the future'. For example, it cannot be trained using SP_Close for any given day, since that would trivially allow it to calculate whether the market closed up on any given day. It can only be provided information that one would know at the start of the day (i.e. data from previous days and the opening data). This rules out High, Low and Close values for the market and the VIX and also the market volume for any given day. Finally, I will limit the analysis to relative returns, not absolute values of the indeces, given that the stock market tends to increase with time and hence comparing the absolute value of the S&P500 over a 20+ year period will not be helpful. \n",
    "\n",
    "In summary, the final data set will consist of only the 20 calculated features and the 1 label, for the 2,678 validated data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the column headers of the current data set\n",
    "print (validated_data.dtypes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the columns that are not one of the 20 features or the label\n",
    "data = validated_data.drop(validated_data.columns[[0,1,2,3,4,5,6,7,8,9,13]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the headers of 'data', to validate that the correct columns were dropped\n",
    "print (data.dtypes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: export the final data set that will be used for training / testing the algorithms\n",
    "np.savetxt(\"eval-data.csv\", data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output basic statistics on the dataset\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate features and labels:\n",
    "\n",
    "X = pd.DataFrame(data.loc[:,'Days_Since_Open':'Trail_1d_VIX_PtT'],dtype='float32')\n",
    "y = pd.DataFrame(data.loc[:,'Intraday_Increase'],dtype='float32')\n",
    "\n",
    "# Summarize the label data\n",
    "display(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Supervised Learning Algorithms:\n",
    "\n",
    "The next few blocks of code will develop and optimize three SKLearn supervised learning classification algorithms, namely Decision Trees (DT), Naive Bayes, and Support Vector Machines (SVM). This code is based on code from project 2 (student intervention) of the Udacity Machine Learning Nanodegree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train, validation and test sets:\n",
    "# Shuffling will not be used because test data should be drawn from the latest data points, to remove any risk of\n",
    "# the learning algorithms glimpsing the future\n",
    "\n",
    "X_train = X[252:]\n",
    "X_test = X[:252]\n",
    "\n",
    "y_train = y[252:]\n",
    "y_test = y[:252]\n",
    "\n",
    "y_train = np.reshape(y_train.values,[2426,])\n",
    "y_test = np.reshape(y_test.values,[252,])\n",
    "\n",
    "describe_train = pd.DataFrame(X_train)\n",
    "describe_test = pd.DataFrame(X_test)\n",
    "\n",
    "describe_train_labels = pd.DataFrame(y_train)\n",
    "describe_test_labels = pd.DataFrame(y_test)\n",
    "\n",
    "\n",
    "display(describe_train.describe())\n",
    "display(describe_test.describe())\n",
    "\n",
    "display(describe_train_labels.describe())\n",
    "display(describe_test_labels.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the benchmark values, against which the learners will be assessed\n",
    "\n",
    "# A suitable benchmark for the models would be comparing to the case where one simply predicts all 1s on the test set\n",
    "# i.e. simply assume the market will go up every day, as it historically has on 52-54% of days in the time range\n",
    "# under consideration here:\n",
    "\n",
    "# Benchmark F1 score results from simply predicting all '1' on the test set\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print (\"F1 score for predicting all \\\"up (1)\\\" on test set: {:.4f}\".format(\n",
    "    f1_score(y_test, [1]*len(y_test), pos_label=1, average='binary')))\n",
    "\n",
    "# Benchmark accuracy score results from simply predicting all '1' on the test set\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (\"Accuracy score for predicting all \\\"up (1)\\\" on test set: {:.4f}\".format(\n",
    "    accuracy_score(y_test, [1]*len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up a series of functions for monitoring training and testing of the various learning algorithms\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print (\"Trained model in {:.4f} seconds\".format(end - start))\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return f1_score(target, y_pred, pos_label=1)\n",
    "\n",
    "def predict_labels_accuracy(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on the accuracy score. '''\n",
    "    \n",
    "    y_pred = clf.predict(features)\n",
    "    \n",
    "    # Print and return results\n",
    "    return accuracy_score(target, y_pred)   \n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print (\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"Accuracy score for training set: {:.4f}.\".format(predict_labels_accuracy(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "    print (\"Accuracy score for test set: {:.4f}.\".format(predict_labels_accuracy(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import  three supervised learning models from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the three models\n",
    "clf_A = DecisionTreeClassifier(random_state=1)\n",
    "clf_B = GaussianNB()\n",
    "clf_C = SVC(random_state=2)\n",
    "\n",
    "# Set up the training set sizes\n",
    "X_train_800 = X_train[:800]\n",
    "y_train_800 = y_train[:800]\n",
    "\n",
    "X_train_1600 = X_train[:1600]\n",
    "y_train_1600 = y_train[:1600]\n",
    "\n",
    "X_train_2426 = X_train[:2426]\n",
    "y_train_2426 = y_train[:2426]\n",
    "\n",
    "# Execute the 'train_predict' function for each classifier and each training set size\n",
    "\n",
    "# loop thru models, then thru train sizes\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    print (\"\\n{}: \\n\".format(clf.__class__.__name__))\n",
    "    for n in [800, 1600, 2426]:\n",
    "        train_predict(clf, X_train[:n], y_train[:n], X_test, y_test)\n",
    "        print ('-'*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimize the Decision Tree parameters by using grid search cross validation\n",
    "\n",
    "# Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Create the parameters list you wish to tune\n",
    "\n",
    "parameters = {'max_depth': [1,2,3,4,5,6,7,8,9],'criterion':('gini','entropy'),'splitter':('best','random')}\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = DecisionTreeClassifier(random_state=4)\n",
    "\n",
    "# Perform grid search on the classifier using the default scoring method (accuracy)\n",
    "grid_obj = GridSearchCV(clf, parameters)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj.fit(X_train,y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a training accuracy score of {:.4f}.\".format(predict_labels_accuracy(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "print (\"Tuned model has a testing accuracy score of {:.4f}.\".format(predict_labels_accuracy(clf, X_test, y_test)))\n",
    "\n",
    "print(\"The best parameters are %s\"\n",
    "      % (grid_obj.best_params_))\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "feature_importance = pd.Series(importances,index=['Days_Since_Open','Break_Coming','Overnight_Return',\n",
    "                                                  'Overnight_VIX','O_to_O','Trail_1d_Ret','Trail_2d_Ret',\n",
    "                                                  'Trail_3d_Ret','Trail_4d_Ret','Trail_5d_Ret','Trail_21d_Ret',\n",
    "                                                  'Trail_63d_Ret','Trail_126d_Ret','Trail_252d_Ret','Trail_1d_VIX',\n",
    "                                                  'Trail_5d_VIX','Trail_1d_Rel_Vol','Trail_5d_Rel_Vol','Trail_1d_PtT',\n",
    "                                                  'Trail_1d_VIX_PtT'])\n",
    "\n",
    "print '\\n'\n",
    "print \"Feature Importance:\" \n",
    "print feature_importance\n",
    "\n",
    "# Output predictions for further analysis\n",
    "np.savetxt(\"eval-DT.csv\", clf.predict(X_test), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimize the SVM parameters by using grid search cross validation\n",
    "\n",
    "# Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Create the parameters list you wish to tune\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf','sigmoid'), 'C':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n",
    "              'gamma':[.000001,.000005,.00005,.0005,.001,.005,.01,.02,.04,.05,.1,.3,.5]}\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = SVC(random_state=3)\n",
    "\n",
    "# Perform grid search on the classifier using the default scoring method (accuracy)\n",
    "grid_obj = GridSearchCV(clf, parameters)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj.fit(X_train,y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a training accuracy score of {:.4f}.\".format(predict_labels_accuracy(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "print (\"Tuned model has a testing accuracy score of {:.4f}.\".format(predict_labels_accuracy(clf, X_test, y_test)))\n",
    "\n",
    "print(\"The best parameters are %s\"\n",
    "      % (grid_obj.best_params_))\n",
    "\n",
    "# Output predictions for further analysis\n",
    "np.savetxt(\"eval-SVM.csv\", clf.predict(X_test), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Algorithm:\n",
    "\n",
    "The next few blocks of code will develop and optimize a TensorFlow based neural network. This code is based on the assignments from the Udacity 'Deep Learning' course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale features (mean = 0, stdev = 1):\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X_scaled = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1-hot encoding on labels (y):\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "y_scaled = enc.transform(y).toarray()\n",
    "\n",
    "y_insight = pd.DataFrame(y_scaled)\n",
    "\n",
    "display(y_insight.describe())\n",
    "\n",
    "y_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train, validation and test sets:\n",
    "# Shuffling will not be used because test data should be drawn from the latest data points, to remove any risk of\n",
    "# the learning algorithms glimpsing the future\n",
    "\n",
    "train_dataset = X_scaled[756:]\n",
    "valid_dataset = X_scaled[252:756]\n",
    "test_dataset = X_scaled[:252]\n",
    "\n",
    "train_labels = y_scaled[756:]\n",
    "valid_labels = y_scaled[252:756]\n",
    "test_labels = y_scaled[:252]\n",
    "\n",
    "tf.cast(train_dataset, tf.float32)\n",
    "tf.cast(valid_dataset, tf.float32)\n",
    "tf.cast(test_dataset, tf.float32)\n",
    "\n",
    "tf.cast(train_labels, tf.float32)\n",
    "tf.cast(valid_labels, tf.float32)\n",
    "tf.cast(test_labels, tf.float32)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "describe_train = pd.DataFrame(train_dataset)\n",
    "describe_valid = pd.DataFrame(valid_dataset)\n",
    "describe_test = pd.DataFrame(test_dataset)\n",
    "\n",
    "describe_train_labels = pd.DataFrame(train_labels)\n",
    "describe_valid_labels = pd.DataFrame(valid_labels)\n",
    "describe_test_labels = pd.DataFrame(test_labels)\n",
    "\n",
    "\n",
    "display(describe_train.describe())\n",
    "display(describe_valid.describe())\n",
    "display(describe_test.describe())\n",
    "\n",
    "display(describe_train_labels.describe())\n",
    "display(describe_valid_labels.describe())\n",
    "display(describe_test_labels.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "#Establish functions for calculating accuracy and F1 score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def F1_score(predictions, labels):\n",
    "  return f1_score(np.argmax(labels, 1), np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Establish TensorFlow Graph\n",
    "\n",
    "batch_size = 1921\n",
    "num_labels = 2\n",
    "num_features = 20\n",
    "\n",
    "# Add second and third hidden layer, each with fewer nodes:\n",
    "\n",
    "num_nodes1 = 16\n",
    "num_nodes2 = 8\n",
    "num_nodes3 = 4\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  tf.set_random_seed(8)\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, num_features))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset,tf.float32)\n",
    "  tf_test_dataset = tf.constant(test_dataset,tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([num_features, num_nodes1],stddev=0.22))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes1]))\n",
    "  \n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes1, num_nodes2],stddev=0.32))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_nodes2]))\n",
    "\n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes2, num_nodes3],stddev=0.45))\n",
    "  biases_3 = tf.Variable(tf.zeros([num_nodes3]))\n",
    "\n",
    "  weights_4 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes3, num_labels],stddev=0.71))\n",
    "  biases_4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation - Layer 1\n",
    "    \n",
    "  hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  \n",
    "  # Adding Dropout - Layer 1\n",
    "    \n",
    "  keep_prob1 = tf.placeholder(tf.float32)\n",
    "  hidden1_drop = tf.nn.dropout(hidden1,keep_prob1)\n",
    "    \n",
    "  # Training computation - Layer 2\n",
    "    \n",
    "  hidden2 = tf.nn.relu(tf.matmul(hidden1_drop, weights_2) + biases_2)\n",
    "  \n",
    "  # Adding Dropout - Layer 2\n",
    "    \n",
    "  keep_prob2 = tf.placeholder(tf.float32)\n",
    "  hidden2_drop = tf.nn.dropout(hidden2,keep_prob2)\n",
    "    \n",
    "  # Training computation - Layer 3\n",
    "    \n",
    "  hidden3 = tf.nn.relu(tf.matmul(hidden2_drop, weights_3) + biases_3)\n",
    "  \n",
    "  # Adding Dropout - Layer 3\n",
    "    \n",
    "  keep_prob3 = tf.placeholder(tf.float32)\n",
    "  hidden3_drop = tf.nn.dropout(hidden3,keep_prob3)\n",
    "    \n",
    "  logits = tf.matmul(hidden3_drop, weights_4) + biases_4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(biases_1) +\n",
    "                  tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(biases_2) +\n",
    "                  tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(biases_3) +\n",
    "                  tf.nn.l2_loss(weights_4) + tf.nn.l2_loss(biases_4))\n",
    "    \n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 1e-6 * regularizers    \n",
    "  \n",
    "  # Optimizer - Add learning rate decay\n",
    "\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "        0.05, global_step, 100000, 0.98, staircase=True)\n",
    "    \n",
    "  optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1)+\n",
    "                                                                       biases_1), weights_2)+ biases_2), weights_3)\n",
    "                             + biases_3), weights_4)+ biases_4)\n",
    "\n",
    "  test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1)+\n",
    "                                                                       biases_1), weights_2)+ biases_2), weights_3)\n",
    "                             + biases_3), weights_4)+ biases_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "\n",
    "# Set train_batches to 8 to use the full training set\n",
    "\n",
    "train_batches = 1\n",
    "train_subset = train_batches * batch_size\n",
    "\n",
    "print('Training set restricted to the following number of samples:',train_subset)\n",
    "\n",
    "train_smallset = train_dataset[0:train_subset,:]\n",
    "train_smalllabel = train_labels[0:train_subset]\n",
    "\n",
    "print('Training set', train_smallset.shape, train_smalllabel.shape)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  step_counter = []\n",
    "  step_train_accuracy = []\n",
    "  step_valid_accuracy = []\n",
    "  step_test_accuracy = []\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "#    offset = (step * batch_size) % (train_smalllabel.shape[0] - batch_size)\n",
    "    offset = 0\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_smallset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_smalllabel[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,keep_prob1: 0.95,\n",
    "                 keep_prob2:0.95,keep_prob3:0.95}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.2f%%\" % accuracy(test_prediction.eval(), test_labels))   \n",
    "      print (\"F1 Score: %f\" % F1_score(test_prediction.eval(), test_labels))\n",
    "      \n",
    "      step_counter.append(step)\n",
    "      step_train_accuracy.append(accuracy(predictions, batch_labels))\n",
    "      step_valid_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "      step_test_accuracy.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    \n",
    "  print '\\n'\n",
    "  print(\"FINAL Test accuracy: %.2f%%\" % accuracy(test_prediction.eval(), test_labels))   \n",
    "  print (\"FINAL F1 Score: %f\" % F1_score(test_prediction.eval(), test_labels))\n",
    "    \n",
    "# Output predictions for further analysis\n",
    "  np.savetxt(\"eval-NN.csv\", test_prediction.eval(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(step_train_accuracy, label = 'train')\n",
    "plt.plot(step_valid_accuracy, label = 'validate')\n",
    "plt.plot(step_test_accuracy, label = 'test')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Number of 500 Step Training Batches')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Simulation\n",
    "\n",
    "The model's performance will now be compared to two benchmark trading strategies, over the 252 day test set (10/01/2015 - 09/29/2016):\n",
    "\n",
    "1. Buy and hold - the index is purchased at the opening price on the first day of the test period and then sold at the closing price of the last day of the test period.\n",
    "\n",
    "2. Buy only - since always buying has a slight (52-54%) accuracy advantage over selling, the index is bought each day at the opening price and then sold at the closing price. This strategy is repeated each day, in contrast to the 'buy and hold' approach, which involves a single buy and a single sell event.\n",
    "\n",
    "The model itself is evaluated as follows: if the model predicts the price will close higher, then the index is bought at the open and sold at the close. If the model predicts the price will close lower, then the index is sold at the open and bought at the close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Identify and format data needed for trading profit calculations\n",
    "\n",
    "validated_data_limited = pd.DataFrame(validated_data.loc[:,'Date':'SP_Close'])\n",
    "validated_data_limited = validated_data_limited[0:252]\n",
    "\n",
    "# Read in the NN Model predictions:\n",
    "ML_trading_data = pd.read_csv(\"eval-NN.csv\",header=None)\n",
    "\n",
    "# If column 1 > column 0, then the label (prediction) is '1' (i.e. S&P500 is predicted to increase that day)\n",
    "ML_trading_data['Predicted_Up'] = ML_trading_data[0] + ML_trading_data[1]\n",
    "\n",
    "ML_trading_data.loc[ML_trading_data[1]>0.5,'Predicted_Up'] = 1\n",
    "ML_trading_data.loc[ML_trading_data[1]<=0.5,'Predicted_Up'] = 0\n",
    "\n",
    "# Append the model predictions to the market data:\n",
    "\n",
    "validated_data_limited['Predicted_Up'] = ML_trading_data['Predicted_Up']\n",
    "\n",
    "# Reverse the dataframe order, so dates go from oldest to newest:\n",
    "validated_data_limited = validated_data_limited.iloc[::-1]\n",
    "\n",
    "# Buy and hold benchmark calculation:\n",
    "\n",
    "validated_data_limited['Cum_Buy_Hold_Profit'] = validated_data_limited['SP_Close'] - (\n",
    "    validated_data_limited['SP_Open'][251])\n",
    "\n",
    "# Buy only benchmark calculation:\n",
    "\n",
    "validated_data_limited['Buy_Only_Profit'] = validated_data_limited['SP_Close'] - validated_data_limited['SP_Open']\n",
    "validated_data_limited['Cum_Buy_Only_Profit'] = np.cumsum(validated_data_limited['Buy_Only_Profit'])\n",
    "\n",
    "# NN model results:\n",
    "# Model profit is equal to the buy only profit, adjusted with a negative sign if the event is a sell instead:\n",
    "\n",
    "validated_data_limited['Model_Profit'] = validated_data_limited['Buy_Only_Profit'] * (\n",
    "    2*validated_data_limited['Predicted_Up'] -1)\n",
    "\n",
    "validated_data_limited['Cum_Model_Profit'] = np.cumsum(validated_data_limited['Model_Profit'])\n",
    "\n",
    "# Output the final cumulative profit for the ML strategy and the benchmarks:\n",
    "\n",
    "print \"Cumulative Buy and Hold Strategy Profit ($):\", validated_data_limited['Cum_Buy_Hold_Profit'][0]\n",
    "print \"Cumulative Buy Only Strategy Profit ($):\", validated_data_limited['Cum_Buy_Only_Profit'][0]\n",
    "print \"Cumulative Machine Learning Strategy Profit ($):\", validated_data_limited['Cum_Model_Profit'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the cumulative performance of the ML trading strategy compared to the benchmarks:\n",
    "\n",
    "trading_data = validated_data_limited\n",
    "\n",
    "plt.axis([252,0,-100,1800])\n",
    "\n",
    "plt.plot(trading_data['Cum_Buy_Hold_Profit'], label = 'buy & hold')\n",
    "plt.plot(trading_data['Cum_Buy_Only_Profit'], label = 'buy only')\n",
    "plt.plot(trading_data['Cum_Model_Profit'], label = 'ML trading')\n",
    "\n",
    "plt.ylabel('Cumulative Profit ($)')\n",
    "plt.xlabel('Look back Days')\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Buy Only Daily Profit Summary Statistics:\"\n",
    "display(validated_data_limited['Buy_Only_Profit'].describe())\n",
    "print '\\n'\n",
    "print \"ML Trading Daily Profit Summary Statistics:\"\n",
    "display(validated_data_limited['Model_Profit'].describe())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
